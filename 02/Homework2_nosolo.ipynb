{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# класс one-hot encoder'а для MNIST\n",
    "class OneHotEncoder:\n",
    "    def __init__(self):\n",
    "        self.transform_mapping = np.zeros((10,10))\n",
    "        for i in range(self.transform_mapping.shape[0]):\n",
    "            self.transform_mapping[i][i] = 1.0\n",
    "    def transform(self, y):\n",
    "        return self.transform_mapping[int(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(X_test, net, fname=\"submission.csv\"):\n",
    "    with open(fname,'w') as fout:\n",
    "        fout.write('Id,Category')\n",
    "        for i in range(X_test.shape[0]):\n",
    "            y_h = net.forward(X_test[i])\n",
    "            y = np.argmax(y_h)\n",
    "            fout.write(\"\\n{},{}\".format(i, int(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# внимание, тут Y_test в обычном формате(не onehot)\n",
    "def compute_acc(X_test, Y_test, net):\n",
    "    acc = 0.0\n",
    "    for i in range(X_test.shape[0]):\n",
    "        y_h = net.forward(X_test[i])\n",
    "        y = np.argmax(y_h)\n",
    "        if(y == Y_test[i]):\n",
    "            acc += 1.0\n",
    "    return acc / Y_test.shape[0]\n",
    "\n",
    "norm_epsilon = 1e-4\n",
    "def data_normalization(data):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    return (data - mean) / (std + norm_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = None\n",
    "test_data = None\n",
    "encoder = OneHotEncoder()\n",
    "with open('data_train.pickle','rb') as fin:\n",
    "    train_data = pickle.load(fin)\n",
    "with open('data_test_no_labels.pickle','rb') as fin:\n",
    "    test_data = pickle.load(fin)\n",
    "    \n",
    "X_train = train_data['data']\n",
    "Y_train = train_data['target']\n",
    "Y_train_oh = np.array(list(map(lambda x : encoder.transform(x), Y_train)))\n",
    "\n",
    "\n",
    "X_test = test_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=5\n",
    "for i in range(n*n):\n",
    "    plt.subplot(n,n,i+1)\n",
    "    I = train_data['data'][np.random.randint(0, X_train.shape[0]),:]\n",
    "    I = I.reshape((28, 28))\n",
    "    plt.imshow(I, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка датасета\n",
    "X_train = data_normalization(X_train)\n",
    "\n",
    "# Splitting data on the training and the test samples\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, random_state=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение слоев сети\n",
    "class Dense:\n",
    "    \n",
    "    def __init__(self, in_size, out_size, rlambda = 0.0):\n",
    "        self.W = np.random.normal(scale=1, size=(out_size, in_size)) * np.sqrt(2 / in_size)\n",
    "        self.b = np.zeros(out_size)\n",
    "        self.rlambda = rlambda\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x # запоминаем для обратного прохода\n",
    "        return np.dot(self.W, x) + self.b\n",
    "    \n",
    "    def get_reg_loss(self):\n",
    "        return 0.5 * self.rlambda * (np.linalg.norm(self.W, ord='fro') ** 2)\n",
    "    \n",
    "    def backward(self, dz, lr=0.001):\n",
    "        # вычисляем градиенты по параметрам (запоминаем их для отладки)\n",
    "        self.dW = np.outer(dz, self.x)\n",
    "        self.db = dz\n",
    "        # вычисляем производную по входу\n",
    "        self.dx = np.matmul(dz, self.W) \n",
    "        # рассчитываем градиенты от регуляризатора\n",
    "        if(self.rlambda != 0):\n",
    "            self.dW += self.rlambda * self.W\n",
    "        # обновляем веса\n",
    "        self.W = self.W - lr * self.dW\n",
    "        self.b = self.b - lr * self.db\n",
    "        # возвращаем dx для продолжения алгоритма\n",
    "        return self.dx\n",
    "    \n",
    "    \n",
    "class ReLU:\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, dz, lr=0.1):\n",
    "        dz[self.x < 0] = 0\n",
    "        return dz\n",
    "    \n",
    "    \n",
    "class Softmax:\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        exps = np.exp(x)\n",
    "        return exps / np.sum(exps)\n",
    "    \n",
    "    def backward(self, dz, lr=0.001):\n",
    "        sm = self.forward(self.x)\n",
    "        self.lp = (np.eye(sm.shape[0], sm.shape[0]) - sm).T\n",
    "        self.lp2 = sm * self.lp\n",
    "        return np.dot(dz, self.lp2)\n",
    "    \n",
    "    \n",
    "class CrossEntropy:\n",
    "    \n",
    "    def forward(self, y_true, y_hat):\n",
    "        self.y_true = y_true\n",
    "        self.y_hat = y_hat\n",
    "        return -1. * np.sum(y_true * np.log(y_hat))\n",
    "    \n",
    "    def backward(self, dz, lr=0.001):\n",
    "        return -1. * dz * self.y_true / self.y_hat\n",
    "    \n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, p = 0.5):\n",
    "        self.p = p\n",
    "        self.train = True\n",
    "    \n",
    "    def set_train(self, train = True):\n",
    "        self.train = train\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.train:\n",
    "            self.mask = np.ones(*x.shape)\n",
    "            return x\n",
    "        self.mask = ( np.random.rand(*x.shape) > self.p ) / (1.0 - self.p)\n",
    "        return x * self.mask\n",
    "        \n",
    "    def backward(self, dz, lr=0.001):\n",
    "        return dz * self.mask\n",
    "    \n",
    "    \n",
    "class MnistNet:\n",
    "    \n",
    "    def __init__(self, rlambda=0.0):\n",
    "        self.d = Dense(784, 2, rlambda)\n",
    "        self.m = ReLU()\n",
    "        self.s = Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        net = self.d.forward(x)\n",
    "        net = self.m.forward(net)\n",
    "        net = self.s.forward(net)\n",
    "        return net\n",
    "    \n",
    "    def backward(self, dz, lr):\n",
    "        dz = self.s.backward(dz, lr)\n",
    "        dz = self.m.backward(dz, lr)\n",
    "        dz = self.d.backward(dz, lr)\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Опеределение самой сети\n",
    "# class MnistNet:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iter loss. Train : 6.168708542021669 . Test : 6.184571470426589\n",
      "1 iter loss. Train : 6.168349894754672 . Test : 6.184571481483927\n",
      "2 iter loss. Train : 6.168349768583222 . Test : 6.184571482786118\n",
      "3 iter loss. Train : 6.168349766835569 . Test : 6.1845714829387175\n",
      "4 iter loss. Train : 6.168349766811535 . Test : 6.184571482956599\n",
      "5 iter loss. Train : 6.168349766811272 . Test : 6.184571482958694\n",
      "6 iter loss. Train : 6.168349766811244 . Test : 6.1845714829589395\n",
      "7 iter loss. Train : 6.168349766811263 . Test : 6.184571482958968\n",
      "8 iter loss. Train : 6.168349766811258 . Test : 6.1845714829589715\n",
      "9 iter loss. Train : 6.168349766811223 . Test : 6.184571482958972\n",
      "10 iter loss. Train : 6.168349766811251 . Test : 6.184571482958972\n",
      "11 iter loss. Train : 6.168349766811229 . Test : 6.184571482958972\n",
      "12 iter loss. Train : 6.168349766811244 . Test : 6.184571482958972\n",
      "13 iter loss. Train : 6.168349766811225 . Test : 6.184571482958972\n",
      "14 iter loss. Train : 6.168349766811261 . Test : 6.184571482958972\n",
      "15 iter loss. Train : 6.168349766811248 . Test : 6.184571482958972\n",
      "16 iter loss. Train : 6.168349766811275 . Test : 6.184571482958972\n",
      "17 iter loss. Train : 6.1683497668112395 . Test : 6.184571482958972\n",
      "18 iter loss. Train : 6.168349766811261 . Test : 6.184571482958972\n",
      "19 iter loss. Train : 6.168349766811218 . Test : 6.184571482958972\n",
      "20 iter loss. Train : 6.1683497668112635 . Test : 6.184571482958972\n",
      "21 iter loss. Train : 6.1683497668112635 . Test : 6.184571482958972\n",
      "22 iter loss. Train : 6.168349766811232 . Test : 6.184571482958972\n",
      "23 iter loss. Train : 6.168349766811283 . Test : 6.184571482958972\n",
      "24 iter loss. Train : 6.1683497668112794 . Test : 6.184571482958972\n",
      "25 iter loss. Train : 6.168349766811306 . Test : 6.184571482958972\n",
      "26 iter loss. Train : 6.168349766811276 . Test : 6.184571482958972\n",
      "27 iter loss. Train : 6.168349766811231 . Test : 6.184571482958972\n",
      "28 iter loss. Train : 6.1683497668112555 . Test : 6.184571482958972\n",
      "29 iter loss. Train : 6.168349766811293 . Test : 6.184571482958972\n",
      "30 iter loss. Train : 6.168349766811241 . Test : 6.184571482958972\n",
      "31 iter loss. Train : 6.1683497668111915 . Test : 6.184571482958972\n",
      "32 iter loss. Train : 6.168349766811262 . Test : 6.184571482958972\n",
      "33 iter loss. Train : 6.168349766811231 . Test : 6.184571482958972\n",
      "34 iter loss. Train : 6.168349766811259 . Test : 6.184571482958972\n",
      "35 iter loss. Train : 6.168349766811215 . Test : 6.184571482958972\n",
      "36 iter loss. Train : 6.168349766811248 . Test : 6.184571482958972\n",
      "37 iter loss. Train : 6.168349766811268 . Test : 6.184571482958972\n",
      "38 iter loss. Train : 6.168349766811262 . Test : 6.184571482958972\n",
      "39 iter loss. Train : 6.168349766811295 . Test : 6.184571482958972\n",
      "40 iter loss. Train : 6.168349766811243 . Test : 6.184571482958972\n",
      "41 iter loss. Train : 6.168349766811245 . Test : 6.184571482958972\n",
      "42 iter loss. Train : 6.168349766811268 . Test : 6.184571482958972\n",
      "43 iter loss. Train : 6.16834976681126 . Test : 6.184571482958972\n",
      "44 iter loss. Train : 6.168349766811233 . Test : 6.184571482958972\n",
      "45 iter loss. Train : 6.1683497668112155 . Test : 6.184571482958972\n",
      "46 iter loss. Train : 6.168349766811258 . Test : 6.184571482958972\n",
      "47 iter loss. Train : 6.1683497668112475 . Test : 6.184571482958972\n",
      "48 iter loss. Train : 6.168349766811245 . Test : 6.184571482958972\n",
      "49 iter loss. Train : 6.168349766811242 . Test : 6.184571482958972\n",
      "50 iter loss. Train : 6.168349766811236 . Test : 6.184571482958972\n",
      "51 iter loss. Train : 6.168349766811274 . Test : 6.184571482958972\n",
      "52 iter loss. Train : 6.16834976681127 . Test : 6.184571482958972\n",
      "53 iter loss. Train : 6.168349766811275 . Test : 6.184571482958972\n",
      "54 iter loss. Train : 6.168349766811321 . Test : 6.184571482958972\n",
      "55 iter loss. Train : 6.168349766811289 . Test : 6.184571482958972\n",
      "56 iter loss. Train : 6.168349766811246 . Test : 6.184571482958972\n",
      "57 iter loss. Train : 6.168349766811284 . Test : 6.184571482958972\n",
      "58 iter loss. Train : 6.168349766811301 . Test : 6.184571482958972\n",
      "59 iter loss. Train : 6.1683497668112715 . Test : 6.184571482958972\n",
      "60 iter loss. Train : 6.168349766811246 . Test : 6.184571482958972\n",
      "61 iter loss. Train : 6.1683497668112475 . Test : 6.184571482958972\n",
      "62 iter loss. Train : 6.168349766811236 . Test : 6.184571482958972\n",
      "63 iter loss. Train : 6.16834976681126 . Test : 6.184571482958972\n",
      "64 iter loss. Train : 6.168349766811271 . Test : 6.184571482958972\n",
      "65 iter loss. Train : 6.16834976681126 . Test : 6.184571482958972\n",
      "66 iter loss. Train : 6.168349766811246 . Test : 6.184571482958972\n",
      "67 iter loss. Train : 6.168349766811261 . Test : 6.184571482958972\n",
      "68 iter loss. Train : 6.168349766811277 . Test : 6.184571482958972\n",
      "69 iter loss. Train : 6.168349766811267 . Test : 6.184571482958972\n",
      "70 iter loss. Train : 6.168349766811285 . Test : 6.184571482958972\n",
      "71 iter loss. Train : 6.1683497668112865 . Test : 6.184571482958972\n",
      "72 iter loss. Train : 6.168349766811257 . Test : 6.184571482958972\n",
      "73 iter loss. Train : 6.168349766811248 . Test : 6.184571482958972\n",
      "74 iter loss. Train : 6.168349766811275 . Test : 6.184571482958972\n",
      "75 iter loss. Train : 6.168349766811244 . Test : 6.184571482958972\n",
      "76 iter loss. Train : 6.168349766811258 . Test : 6.184571482958972\n",
      "77 iter loss. Train : 6.168349766811256 . Test : 6.184571482958972\n",
      "78 iter loss. Train : 6.168349766811234 . Test : 6.184571482958972\n",
      "79 iter loss. Train : 6.168349766811293 . Test : 6.184571482958972\n",
      "80 iter loss. Train : 6.168349766811234 . Test : 6.184571482958972\n",
      "81 iter loss. Train : 6.168349766811236 . Test : 6.184571482958972\n",
      "82 iter loss. Train : 6.16834976681129 . Test : 6.184571482958972\n",
      "83 iter loss. Train : 6.168349766811258 . Test : 6.184571482958972\n",
      "84 iter loss. Train : 6.168349766811234 . Test : 6.184571482958972\n",
      "85 iter loss. Train : 6.168349766811246 . Test : 6.184571482958972\n",
      "86 iter loss. Train : 6.168349766811258 . Test : 6.184571482958972\n",
      "87 iter loss. Train : 6.16834976681125 . Test : 6.184571482958972\n",
      "88 iter loss. Train : 6.168349766811225 . Test : 6.184571482958972\n",
      "89 iter loss. Train : 6.168349766811241 . Test : 6.184571482958972\n",
      "90 iter loss. Train : 6.168349766811313 . Test : 6.184571482958972\n",
      "91 iter loss. Train : 6.168349766811267 . Test : 6.184571482958972\n",
      "92 iter loss. Train : 6.1683497668112865 . Test : 6.184571482958972\n",
      "93 iter loss. Train : 6.168349766811239 . Test : 6.184571482958972\n",
      "94 iter loss. Train : 6.168349766811234 . Test : 6.184571482958972\n",
      "95 iter loss. Train : 6.168349766811235 . Test : 6.184571482958972\n",
      "96 iter loss. Train : 6.168349766811268 . Test : 6.184571482958972\n",
      "97 iter loss. Train : 6.168349766811267 . Test : 6.184571482958972\n",
      "98 iter loss. Train : 6.168349766811282 . Test : 6.184571482958972\n",
      "99 iter loss. Train : 6.16834976681128 . Test : 6.184571482958972\n",
      "100 iter loss. Train : 6.168349766811281 . Test : 6.184571482958972\n",
      "101 iter loss. Train : 6.168349766811257 . Test : 6.184571482958972\n",
      "102 iter loss. Train : 6.168349766811271 . Test : 6.184571482958972\n",
      "103 iter loss. Train : 6.1683497668112475 . Test : 6.184571482958972\n",
      "104 iter loss. Train : 6.168349766811242 . Test : 6.184571482958972\n",
      "105 iter loss. Train : 6.168349766811247 . Test : 6.184571482958972\n",
      "106 iter loss. Train : 6.168349766811259 . Test : 6.184571482958972\n",
      "107 iter loss. Train : 6.168349766811251 . Test : 6.184571482958972\n",
      "108 iter loss. Train : 6.168349766811208 . Test : 6.184571482958972\n",
      "109 iter loss. Train : 6.168349766811284 . Test : 6.184571482958972\n",
      "110 iter loss. Train : 6.16834976681124 . Test : 6.184571482958972\n",
      "111 iter loss. Train : 6.1683497668112155 . Test : 6.184571482958972\n",
      "112 iter loss. Train : 6.168349766811291 . Test : 6.184571482958972\n",
      "113 iter loss. Train : 6.168349766811238 . Test : 6.184571482958972\n",
      "114 iter loss. Train : 6.168349766811235 . Test : 6.184571482958972\n",
      "115 iter loss. Train : 6.168349766811317 . Test : 6.184571482958972\n",
      "116 iter loss. Train : 6.168349766811257 . Test : 6.184571482958972\n",
      "117 iter loss. Train : 6.168349766811233 . Test : 6.184571482958972\n",
      "118 iter loss. Train : 6.1683497668112794 . Test : 6.184571482958972\n",
      "119 iter loss. Train : 6.168349766811238 . Test : 6.184571482958972\n",
      "120 iter loss. Train : 6.168349766811268 . Test : 6.184571482958972\n",
      "121 iter loss. Train : 6.168349766811267 . Test : 6.184571482958972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 iter loss. Train : 6.168349766811235 . Test : 6.184571482958972\n",
      "123 iter loss. Train : 6.168349766811248 . Test : 6.184571482958972\n",
      "124 iter loss. Train : 6.168349766811261 . Test : 6.184571482958972\n",
      "125 iter loss. Train : 6.168349766811257 . Test : 6.184571482958972\n",
      "126 iter loss. Train : 6.168349766811264 . Test : 6.184571482958972\n",
      "127 iter loss. Train : 6.168349766811238 . Test : 6.184571482958972\n",
      "128 iter loss. Train : 6.168349766811268 . Test : 6.184571482958972\n",
      "129 iter loss. Train : 6.168349766811264 . Test : 6.184571482958972\n",
      "130 iter loss. Train : 6.168349766811251 . Test : 6.184571482958972\n",
      "131 iter loss. Train : 6.168349766811206 . Test : 6.184571482958972\n",
      "132 iter loss. Train : 6.1683497668112395 . Test : 6.184571482958972\n",
      "133 iter loss. Train : 6.168349766811288 . Test : 6.184571482958972\n",
      "134 iter loss. Train : 6.168349766811253 . Test : 6.184571482958972\n",
      "135 iter loss. Train : 6.168349766811266 . Test : 6.184571482958972\n",
      "136 iter loss. Train : 6.168349766811228 . Test : 6.184571482958972\n",
      "137 iter loss. Train : 6.168349766811237 . Test : 6.184571482958972\n",
      "138 iter loss. Train : 6.168349766811229 . Test : 6.184571482958972\n",
      "139 iter loss. Train : 6.168349766811262 . Test : 6.184571482958972\n",
      "140 iter loss. Train : 6.168349766811266 . Test : 6.184571482958972\n",
      "141 iter loss. Train : 6.168349766811223 . Test : 6.184571482958972\n",
      "142 iter loss. Train : 6.168349766811294 . Test : 6.184571482958972\n",
      "143 iter loss. Train : 6.168349766811258 . Test : 6.184571482958972\n",
      "144 iter loss. Train : 6.168349766811254 . Test : 6.184571482958972\n",
      "145 iter loss. Train : 6.168349766811271 . Test : 6.184571482958972\n",
      "146 iter loss. Train : 6.168349766811252 . Test : 6.184571482958972\n",
      "147 iter loss. Train : 6.168349766811287 . Test : 6.184571482958972\n",
      "148 iter loss. Train : 6.168349766811253 . Test : 6.184571482958972\n",
      "149 iter loss. Train : 6.168349766811262 . Test : 6.184571482958972\n",
      "150 iter loss. Train : 6.168349766811253 . Test : 6.184571482958972\n",
      "151 iter loss. Train : 6.168349766811281 . Test : 6.184571482958972\n",
      "152 iter loss. Train : 6.168349766811251 . Test : 6.184571482958972\n",
      "153 iter loss. Train : 6.168349766811263 . Test : 6.184571482958972\n",
      "154 iter loss. Train : 6.168349766811304 . Test : 6.184571482958972\n",
      "155 iter loss. Train : 6.168349766811279 . Test : 6.184571482958972\n",
      "156 iter loss. Train : 6.168349766811278 . Test : 6.184571482958972\n",
      "157 iter loss. Train : 6.168349766811265 . Test : 6.184571482958972\n",
      "158 iter loss. Train : 6.168349766811246 . Test : 6.184571482958972\n",
      "159 iter loss. Train : 6.168349766811234 . Test : 6.184571482958972\n",
      "160 iter loss. Train : 6.1683497668112395 . Test : 6.184571482958972\n",
      "161 iter loss. Train : 6.168349766811251 . Test : 6.184571482958972\n",
      "162 iter loss. Train : 6.168349766811272 . Test : 6.184571482958972\n",
      "163 iter loss. Train : 6.168349766811231 . Test : 6.184571482958972\n",
      "164 iter loss. Train : 6.168349766811205 . Test : 6.184571482958972\n",
      "165 iter loss. Train : 6.168349766811236 . Test : 6.184571482958972\n",
      "166 iter loss. Train : 6.1683497668112315 . Test : 6.184571482958972\n",
      "167 iter loss. Train : 6.168349766811297 . Test : 6.184571482958972\n",
      "168 iter loss. Train : 6.168349766811285 . Test : 6.184571482958972\n",
      "169 iter loss. Train : 6.168349766811263 . Test : 6.184571482958972\n",
      "170 iter loss. Train : 6.168349766811242 . Test : 6.184571482958972\n",
      "171 iter loss. Train : 6.168349766811253 . Test : 6.184571482958972\n",
      "172 iter loss. Train : 6.1683497668112155 . Test : 6.184571482958972\n",
      "173 iter loss. Train : 6.1683497668112865 . Test : 6.184571482958972\n",
      "174 iter loss. Train : 6.168349766811241 . Test : 6.184571482958972\n",
      "175 iter loss. Train : 6.168349766811197 . Test : 6.184571482958972\n",
      "176 iter loss. Train : 6.168349766811271 . Test : 6.184571482958972\n",
      "177 iter loss. Train : 6.168349766811268 . Test : 6.184571482958972\n",
      "178 iter loss. Train : 6.168349766811238 . Test : 6.184571482958972\n",
      "179 iter loss. Train : 6.168349766811258 . Test : 6.184571482958972\n",
      "180 iter loss. Train : 6.1683497668112395 . Test : 6.184571482958972\n",
      "181 iter loss. Train : 6.168349766811254 . Test : 6.184571482958972\n",
      "182 iter loss. Train : 6.168349766811262 . Test : 6.184571482958972\n",
      "183 iter loss. Train : 6.168349766811235 . Test : 6.184571482958972\n",
      "184 iter loss. Train : 6.168349766811266 . Test : 6.184571482958972\n",
      "185 iter loss. Train : 6.1683497668112395 . Test : 6.184571482958972\n",
      "186 iter loss. Train : 6.168349766811255 . Test : 6.184571482958972\n",
      "187 iter loss. Train : 6.168349766811252 . Test : 6.184571482958972\n",
      "188 iter loss. Train : 6.168349766811235 . Test : 6.184571482958972\n",
      "189 iter loss. Train : 6.168349766811299 . Test : 6.184571482958972\n",
      "190 iter loss. Train : 6.168349766811264 . Test : 6.184571482958972\n",
      "191 iter loss. Train : 6.168349766811248 . Test : 6.184571482958972\n",
      "192 iter loss. Train : 6.168349766811272 . Test : 6.184571482958972\n",
      "193 iter loss. Train : 6.168349766811269 . Test : 6.184571482958972\n",
      "194 iter loss. Train : 6.168349766811245 . Test : 6.184571482958972\n",
      "195 iter loss. Train : 6.168349766811259 . Test : 6.184571482958972\n",
      "196 iter loss. Train : 6.168349766811292 . Test : 6.184571482958972\n",
      "197 iter loss. Train : 6.16834976681122 . Test : 6.184571482958972\n",
      "198 iter loss. Train : 6.168349766811264 . Test : 6.184571482958972\n",
      "199 iter loss. Train : 6.168349766811239 . Test : 6.184571482958972\n",
      "200 iter loss. Train : 6.16834976681125 . Test : 6.184571482958972\n",
      "201 iter loss. Train : 6.1683497668112715 . Test : 6.184571482958972\n",
      "202 iter loss. Train : 6.168349766811265 . Test : 6.184571482958972\n",
      "203 iter loss. Train : 6.168349766811284 . Test : 6.184571482958972\n",
      "204 iter loss. Train : 6.168349766811262 . Test : 6.184571482958972\n",
      "205 iter loss. Train : 6.168349766811262 . Test : 6.184571482958972\n",
      "206 iter loss. Train : 6.168349766811275 . Test : 6.184571482958972\n",
      "207 iter loss. Train : 6.168349766811244 . Test : 6.184571482958972\n",
      "208 iter loss. Train : 6.168349766811275 . Test : 6.184571482958972\n",
      "209 iter loss. Train : 6.16834976681125 . Test : 6.184571482958972\n",
      "210 iter loss. Train : 6.168349766811297 . Test : 6.184571482958972\n",
      "211 iter loss. Train : 6.168349766811261 . Test : 6.184571482958972\n",
      "212 iter loss. Train : 6.168349766811274 . Test : 6.184571482958972\n",
      "213 iter loss. Train : 6.168349766811275 . Test : 6.184571482958972\n",
      "214 iter loss. Train : 6.168349766811244 . Test : 6.184571482958972\n",
      "215 iter loss. Train : 6.168349766811275 . Test : 6.184571482958972\n",
      "216 iter loss. Train : 6.1683497668112555 . Test : 6.184571482958972\n",
      "217 iter loss. Train : 6.168349766811229 . Test : 6.184571482958972\n",
      "218 iter loss. Train : 6.16834976681124 . Test : 6.184571482958972\n",
      "219 iter loss. Train : 6.168349766811266 . Test : 6.184571482958972\n",
      "220 iter loss. Train : 6.168349766811264 . Test : 6.184571482958972\n",
      "221 iter loss. Train : 6.168349766811276 . Test : 6.184571482958972\n",
      "222 iter loss. Train : 6.168349766811293 . Test : 6.184571482958972\n",
      "223 iter loss. Train : 6.168349766811261 . Test : 6.184571482958972\n",
      "224 iter loss. Train : 6.168349766811239 . Test : 6.184571482958972\n",
      "225 iter loss. Train : 6.168349766811279 . Test : 6.184571482958972\n",
      "226 iter loss. Train : 6.168349766811215 . Test : 6.184571482958972\n",
      "227 iter loss. Train : 6.168349766811243 . Test : 6.184571482958972\n",
      "228 iter loss. Train : 6.168349766811247 . Test : 6.184571482958972\n",
      "229 iter loss. Train : 6.168349766811252 . Test : 6.184571482958972\n",
      "230 iter loss. Train : 6.168349766811283 . Test : 6.184571482958972\n",
      "231 iter loss. Train : 6.168349766811312 . Test : 6.184571482958972\n",
      "232 iter loss. Train : 6.168349766811274 . Test : 6.184571482958972\n",
      "233 iter loss. Train : 6.168349766811244 . Test : 6.184571482958972\n",
      "234 iter loss. Train : 6.1683497668112395 . Test : 6.184571482958972\n",
      "235 iter loss. Train : 6.168349766811233 . Test : 6.184571482958972\n",
      "236 iter loss. Train : 6.168349766811264 . Test : 6.184571482958972\n",
      "237 iter loss. Train : 6.168349766811238 . Test : 6.184571482958972\n",
      "238 iter loss. Train : 6.168349766811235 . Test : 6.184571482958972\n",
      "239 iter loss. Train : 6.168349766811241 . Test : 6.184571482958972\n",
      "240 iter loss. Train : 6.1683497668112235 . Test : 6.184571482958972\n",
      "241 iter loss. Train : 6.1683497668112395 . Test : 6.184571482958972\n",
      "242 iter loss. Train : 6.168349766811244 . Test : 6.184571482958972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243 iter loss. Train : 6.168349766811256 . Test : 6.184571482958972\n",
      "244 iter loss. Train : 6.168349766811255 . Test : 6.184571482958972\n",
      "245 iter loss. Train : 6.16834976681123 . Test : 6.184571482958972\n",
      "246 iter loss. Train : 6.16834976681122 . Test : 6.184571482958972\n",
      "247 iter loss. Train : 6.168349766811266 . Test : 6.184571482958972\n",
      "248 iter loss. Train : 6.168349766811253 . Test : 6.184571482958972\n",
      "249 iter loss. Train : 6.16834976681124 . Test : 6.184571482958972\n",
      "250 iter loss. Train : 6.168349766811236 . Test : 6.184571482958972\n",
      "251 iter loss. Train : 6.168349766811248 . Test : 6.184571482958972\n",
      "252 iter loss. Train : 6.168349766811246 . Test : 6.184571482958972\n",
      "253 iter loss. Train : 6.168349766811253 . Test : 6.184571482958972\n",
      "254 iter loss. Train : 6.168349766811253 . Test : 6.184571482958972\n",
      "255 iter loss. Train : 6.168349766811247 . Test : 6.184571482958972\n",
      "256 iter loss. Train : 6.168349766811224 . Test : 6.184571482958972\n",
      "257 iter loss. Train : 6.1683497668112155 . Test : 6.184571482958972\n",
      "258 iter loss. Train : 6.168349766811226 . Test : 6.184571482958972\n",
      "259 iter loss. Train : 6.168349766811254 . Test : 6.184571482958972\n",
      "260 iter loss. Train : 6.168349766811237 . Test : 6.184571482958972\n",
      "261 iter loss. Train : 6.168349766811229 . Test : 6.184571482958972\n",
      "262 iter loss. Train : 6.1683497668113425 . Test : 6.184571482958972\n",
      "263 iter loss. Train : 6.168349766811275 . Test : 6.184571482958972\n",
      "264 iter loss. Train : 6.168349766811281 . Test : 6.184571482958972\n",
      "265 iter loss. Train : 6.168349766811275 . Test : 6.184571482958972\n",
      "266 iter loss. Train : 6.168349766811223 . Test : 6.184571482958972\n",
      "267 iter loss. Train : 6.168349766811235 . Test : 6.184571482958972\n",
      "268 iter loss. Train : 6.168349766811271 . Test : 6.184571482958972\n",
      "269 iter loss. Train : 6.168349766811258 . Test : 6.184571482958972\n",
      "270 iter loss. Train : 6.168349766811252 . Test : 6.184571482958972\n",
      "271 iter loss. Train : 6.168349766811262 . Test : 6.184571482958972\n",
      "272 iter loss. Train : 6.168349766811237 . Test : 6.184571482958972\n",
      "273 iter loss. Train : 6.168349766811244 . Test : 6.184571482958972\n",
      "274 iter loss. Train : 6.168349766811275 . Test : 6.184571482958972\n",
      "275 iter loss. Train : 6.168349766811262 . Test : 6.184571482958972\n",
      "276 iter loss. Train : 6.168349766811241 . Test : 6.184571482958972\n",
      "277 iter loss. Train : 6.168349766811209 . Test : 6.184571482958972\n",
      "278 iter loss. Train : 6.168349766811257 . Test : 6.184571482958972\n",
      "279 iter loss. Train : 6.168349766811262 . Test : 6.184571482958972\n",
      "280 iter loss. Train : 6.168349766811243 . Test : 6.184571482958972\n",
      "281 iter loss. Train : 6.168349766811244 . Test : 6.184571482958972\n",
      "282 iter loss. Train : 6.168349766811222 . Test : 6.184571482958972\n",
      "283 iter loss. Train : 6.168349766811268 . Test : 6.184571482958972\n",
      "284 iter loss. Train : 6.168349766811285 . Test : 6.184571482958972\n",
      "285 iter loss. Train : 6.168349766811241 . Test : 6.184571482958972\n",
      "286 iter loss. Train : 6.16834976681125 . Test : 6.184571482958972\n",
      "287 iter loss. Train : 6.168349766811244 . Test : 6.184571482958972\n",
      "288 iter loss. Train : 6.168349766811255 . Test : 6.184571482958972\n",
      "289 iter loss. Train : 6.168349766811235 . Test : 6.184571482958972\n",
      "290 iter loss. Train : 6.168349766811237 . Test : 6.184571482958972\n",
      "291 iter loss. Train : 6.16834976681127 . Test : 6.184571482958972\n",
      "292 iter loss. Train : 6.168349766811246 . Test : 6.184571482958972\n",
      "293 iter loss. Train : 6.168349766811288 . Test : 6.184571482958972\n",
      "294 iter loss. Train : 6.168349766811263 . Test : 6.184571482958972\n",
      "295 iter loss. Train : 6.168349766811245 . Test : 6.184571482958972\n",
      "296 iter loss. Train : 6.168349766811237 . Test : 6.184571482958972\n",
      "297 iter loss. Train : 6.168349766811285 . Test : 6.184571482958972\n",
      "298 iter loss. Train : 6.168349766811281 . Test : 6.184571482958972\n",
      "299 iter loss. Train : 6.1683497668112945 . Test : 6.184571482958972\n",
      "300 iter loss. Train : 6.168349766811277 . Test : 6.184571482958972\n",
      "301 iter loss. Train : 6.168349766811215 . Test : 6.184571482958972\n",
      "302 iter loss. Train : 6.168349766811256 . Test : 6.184571482958972\n",
      "303 iter loss. Train : 6.168349766811275 . Test : 6.184571482958972\n",
      "304 iter loss. Train : 6.168349766811239 . Test : 6.184571482958972\n",
      "305 iter loss. Train : 6.168349766811244 . Test : 6.184571482958972\n",
      "306 iter loss. Train : 6.168349766811261 . Test : 6.184571482958972\n",
      "307 iter loss. Train : 6.168349766811231 . Test : 6.184571482958972\n",
      "308 iter loss. Train : 6.168349766811275 . Test : 6.184571482958972\n",
      "309 iter loss. Train : 6.168349766811261 . Test : 6.184571482958972\n",
      "310 iter loss. Train : 6.168349766811266 . Test : 6.184571482958972\n",
      "311 iter loss. Train : 6.168349766811262 . Test : 6.184571482958972\n",
      "312 iter loss. Train : 6.168349766811244 . Test : 6.184571482958972\n",
      "313 iter loss. Train : 6.1683497668112635 . Test : 6.184571482958972\n",
      "314 iter loss. Train : 6.168349766811238 . Test : 6.184571482958972\n",
      "315 iter loss. Train : 6.168349766811256 . Test : 6.184571482958972\n",
      "316 iter loss. Train : 6.168349766811296 . Test : 6.184571482958972\n",
      "317 iter loss. Train : 6.168349766811266 . Test : 6.184571482958972\n",
      "318 iter loss. Train : 6.168349766811231 . Test : 6.184571482958972\n",
      "319 iter loss. Train : 6.168349766811274 . Test : 6.184571482958972\n",
      "320 iter loss. Train : 6.168349766811255 . Test : 6.184571482958972\n",
      "321 iter loss. Train : 6.168349766811279 . Test : 6.184571482958972\n",
      "322 iter loss. Train : 6.168349766811231 . Test : 6.184571482958972\n",
      "323 iter loss. Train : 6.168349766811223 . Test : 6.184571482958972\n",
      "324 iter loss. Train : 6.168349766811259 . Test : 6.184571482958972\n",
      "325 iter loss. Train : 6.168349766811264 . Test : 6.184571482958972\n",
      "326 iter loss. Train : 6.168349766811257 . Test : 6.184571482958972\n",
      "327 iter loss. Train : 6.168349766811254 . Test : 6.184571482958972\n",
      "328 iter loss. Train : 6.168349766811266 . Test : 6.184571482958972\n",
      "329 iter loss. Train : 6.168349766811262 . Test : 6.184571482958972\n",
      "330 iter loss. Train : 6.168349766811243 . Test : 6.184571482958972\n",
      "331 iter loss. Train : 6.168349766811263 . Test : 6.184571482958972\n",
      "332 iter loss. Train : 6.168349766811272 . Test : 6.184571482958972\n",
      "333 iter loss. Train : 6.168349766811238 . Test : 6.184571482958972\n",
      "334 iter loss. Train : 6.168349766811266 . Test : 6.184571482958972\n",
      "335 iter loss. Train : 6.16834976681124 . Test : 6.184571482958972\n",
      "336 iter loss. Train : 6.168349766811253 . Test : 6.184571482958972\n",
      "337 iter loss. Train : 6.168349766811219 . Test : 6.184571482958972\n",
      "338 iter loss. Train : 6.168349766811256 . Test : 6.184571482958972\n",
      "339 iter loss. Train : 6.168349766811251 . Test : 6.184571482958972\n",
      "340 iter loss. Train : 6.168349766811248 . Test : 6.184571482958972\n",
      "341 iter loss. Train : 6.168349766811269 . Test : 6.184571482958972\n",
      "342 iter loss. Train : 6.168349766811284 . Test : 6.184571482958972\n",
      "343 iter loss. Train : 6.168349766811274 . Test : 6.184571482958972\n",
      "344 iter loss. Train : 6.168349766811271 . Test : 6.184571482958972\n",
      "345 iter loss. Train : 6.168349766811276 . Test : 6.184571482958972\n",
      "346 iter loss. Train : 6.168349766811267 . Test : 6.184571482958972\n",
      "347 iter loss. Train : 6.168349766811268 . Test : 6.184571482958972\n",
      "348 iter loss. Train : 6.168349766811288 . Test : 6.184571482958972\n",
      "349 iter loss. Train : 6.168349766811235 . Test : 6.184571482958972\n",
      "350 iter loss. Train : 6.16834976681126 . Test : 6.184571482958972\n",
      "351 iter loss. Train : 6.168349766811255 . Test : 6.184571482958972\n",
      "352 iter loss. Train : 6.168349766811248 . Test : 6.184571482958972\n",
      "353 iter loss. Train : 6.168349766811238 . Test : 6.184571482958972\n",
      "354 iter loss. Train : 6.168349766811223 . Test : 6.184571482958972\n",
      "355 iter loss. Train : 6.168349766811222 . Test : 6.184571482958972\n",
      "356 iter loss. Train : 6.168349766811253 . Test : 6.184571482958972\n",
      "357 iter loss. Train : 6.168349766811268 . Test : 6.184571482958972\n",
      "358 iter loss. Train : 6.168349766811266 . Test : 6.184571482958972\n",
      "359 iter loss. Train : 6.168349766811248 . Test : 6.184571482958972\n",
      "360 iter loss. Train : 6.168349766811229 . Test : 6.184571482958972\n",
      "361 iter loss. Train : 6.168349766811218 . Test : 6.184571482958972\n",
      "362 iter loss. Train : 6.168349766811214 . Test : 6.184571482958972\n",
      "363 iter loss. Train : 6.168349766811252 . Test : 6.184571482958972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364 iter loss. Train : 6.168349766811196 . Test : 6.184571482958972\n",
      "365 iter loss. Train : 6.168349766811214 . Test : 6.184571482958972\n",
      "366 iter loss. Train : 6.168349766811244 . Test : 6.184571482958972\n",
      "367 iter loss. Train : 6.168349766811277 . Test : 6.184571482958972\n",
      "368 iter loss. Train : 6.168349766811237 . Test : 6.184571482958972\n",
      "369 iter loss. Train : 6.168349766811247 . Test : 6.184571482958972\n",
      "370 iter loss. Train : 6.1683497668112715 . Test : 6.184571482958972\n",
      "371 iter loss. Train : 6.168349766811256 . Test : 6.184571482958972\n",
      "372 iter loss. Train : 6.168349766811237 . Test : 6.184571482958972\n",
      "373 iter loss. Train : 6.16834976681124 . Test : 6.184571482958972\n",
      "374 iter loss. Train : 6.168349766811275 . Test : 6.184571482958972\n",
      "375 iter loss. Train : 6.168349766811244 . Test : 6.184571482958972\n",
      "376 iter loss. Train : 6.168349766811253 . Test : 6.184571482958972\n",
      "377 iter loss. Train : 6.168349766811248 . Test : 6.184571482958972\n",
      "378 iter loss. Train : 6.168349766811272 . Test : 6.184571482958972\n",
      "379 iter loss. Train : 6.168349766811271 . Test : 6.184571482958972\n",
      "380 iter loss. Train : 6.168349766811253 . Test : 6.184571482958972\n",
      "381 iter loss. Train : 6.168349766811302 . Test : 6.184571482958972\n",
      "382 iter loss. Train : 6.1683497668112235 . Test : 6.184571482958972\n",
      "383 iter loss. Train : 6.168349766811266 . Test : 6.184571482958972\n",
      "384 iter loss. Train : 6.168349766811269 . Test : 6.184571482958972\n",
      "385 iter loss. Train : 6.168349766811284 . Test : 6.184571482958972\n",
      "386 iter loss. Train : 6.168349766811241 . Test : 6.184571482958972\n",
      "387 iter loss. Train : 6.168349766811219 . Test : 6.184571482958972\n",
      "388 iter loss. Train : 6.168349766811267 . Test : 6.184571482958972\n",
      "389 iter loss. Train : 6.168349766811278 . Test : 6.184571482958972\n",
      "390 iter loss. Train : 6.168349766811227 . Test : 6.184571482958972\n",
      "391 iter loss. Train : 6.168349766811302 . Test : 6.184571482958972\n",
      "392 iter loss. Train : 6.168349766811237 . Test : 6.184571482958972\n",
      "393 iter loss. Train : 6.168349766811255 . Test : 6.184571482958972\n",
      "394 iter loss. Train : 6.168349766811282 . Test : 6.184571482958972\n",
      "395 iter loss. Train : 6.168349766811219 . Test : 6.184571482958972\n",
      "396 iter loss. Train : 6.168349766811235 . Test : 6.184571482958972\n",
      "397 iter loss. Train : 6.168349766811281 . Test : 6.184571482958972\n",
      "398 iter loss. Train : 6.168349766811237 . Test : 6.184571482958972\n",
      "399 iter loss. Train : 6.168349766811235 . Test : 6.184571482958972\n",
      "400 iter loss. Train : 6.168349766811275 . Test : 6.184571482958972\n",
      "401 iter loss. Train : 6.1683497668112395 . Test : 6.184571482958972\n",
      "402 iter loss. Train : 6.168349766811211 . Test : 6.184571482958972\n",
      "403 iter loss. Train : 6.168349766811246 . Test : 6.184571482958972\n",
      "404 iter loss. Train : 6.168349766811212 . Test : 6.184571482958972\n",
      "405 iter loss. Train : 6.1683497668112475 . Test : 6.184571482958972\n",
      "406 iter loss. Train : 6.1683497668112555 . Test : 6.184571482958972\n",
      "407 iter loss. Train : 6.168349766811297 . Test : 6.184571482958972\n",
      "408 iter loss. Train : 6.168349766811202 . Test : 6.184571482958972\n",
      "409 iter loss. Train : 6.168349766811239 . Test : 6.184571482958972\n",
      "410 iter loss. Train : 6.1683497668112395 . Test : 6.184571482958972\n",
      "411 iter loss. Train : 6.168349766811253 . Test : 6.184571482958972\n",
      "412 iter loss. Train : 6.168349766811245 . Test : 6.184571482958972\n",
      "413 iter loss. Train : 6.16834976681119 . Test : 6.184571482958972\n",
      "414 iter loss. Train : 6.168349766811268 . Test : 6.184571482958972\n",
      "415 iter loss. Train : 6.1683497668112475 . Test : 6.184571482958972\n",
      "416 iter loss. Train : 6.168349766811244 . Test : 6.184571482958972\n",
      "417 iter loss. Train : 6.168349766811268 . Test : 6.184571482958972\n",
      "418 iter loss. Train : 6.1683497668113025 . Test : 6.184571482958972\n",
      "419 iter loss. Train : 6.1683497668112395 . Test : 6.184571482958972\n",
      "420 iter loss. Train : 6.168349766811289 . Test : 6.184571482958972\n",
      "421 iter loss. Train : 6.168349766811223 . Test : 6.184571482958972\n",
      "422 iter loss. Train : 6.168349766811277 . Test : 6.184571482958972\n",
      "423 iter loss. Train : 6.16834976681127 . Test : 6.184571482958972\n",
      "424 iter loss. Train : 6.168349766811244 . Test : 6.184571482958972\n",
      "425 iter loss. Train : 6.168349766811218 . Test : 6.184571482958972\n",
      "426 iter loss. Train : 6.168349766811262 . Test : 6.184571482958972\n",
      "427 iter loss. Train : 6.168349766811259 . Test : 6.184571482958972\n",
      "428 iter loss. Train : 6.168349766811243 . Test : 6.184571482958972\n",
      "429 iter loss. Train : 6.168349766811232 . Test : 6.184571482958972\n",
      "430 iter loss. Train : 6.168349766811266 . Test : 6.184571482958972\n",
      "431 iter loss. Train : 6.168349766811271 . Test : 6.184571482958972\n",
      "432 iter loss. Train : 6.1683497668112555 . Test : 6.184571482958972\n",
      "433 iter loss. Train : 6.1683497668112794 . Test : 6.184571482958972\n",
      "434 iter loss. Train : 6.168349766811214 . Test : 6.184571482958972\n",
      "435 iter loss. Train : 6.168349766811252 . Test : 6.184571482958972\n",
      "436 iter loss. Train : 6.1683497668112475 . Test : 6.184571482958972\n",
      "437 iter loss. Train : 6.168349766811228 . Test : 6.184571482958972\n",
      "438 iter loss. Train : 6.168349766811247 . Test : 6.184571482958972\n",
      "439 iter loss. Train : 6.168349766811233 . Test : 6.184571482958972\n",
      "440 iter loss. Train : 6.168349766811253 . Test : 6.184571482958972\n",
      "441 iter loss. Train : 6.16834976681126 . Test : 6.184571482958972\n",
      "442 iter loss. Train : 6.168349766811288 . Test : 6.184571482958972\n",
      "443 iter loss. Train : 6.168349766811216 . Test : 6.184571482958972\n",
      "444 iter loss. Train : 6.168349766811249 . Test : 6.184571482958972\n",
      "445 iter loss. Train : 6.168349766811285 . Test : 6.184571482958972\n",
      "446 iter loss. Train : 6.168349766811281 . Test : 6.184571482958972\n",
      "447 iter loss. Train : 6.168349766811211 . Test : 6.184571482958972\n",
      "448 iter loss. Train : 6.168349766811226 . Test : 6.184571482958972\n",
      "449 iter loss. Train : 6.168349766811309 . Test : 6.184571482958972\n",
      "450 iter loss. Train : 6.168349766811289 . Test : 6.184571482958972\n",
      "451 iter loss. Train : 6.1683497668112555 . Test : 6.184571482958972\n",
      "452 iter loss. Train : 6.1683497668112395 . Test : 6.184571482958972\n",
      "453 iter loss. Train : 6.1683497668112555 . Test : 6.184571482958972\n",
      "454 iter loss. Train : 6.168349766811278 . Test : 6.184571482958972\n",
      "455 iter loss. Train : 6.168349766811273 . Test : 6.184571482958972\n",
      "456 iter loss. Train : 6.1683497668112635 . Test : 6.184571482958972\n",
      "457 iter loss. Train : 6.168349766811277 . Test : 6.184571482958972\n",
      "458 iter loss. Train : 6.168349766811234 . Test : 6.184571482958972\n",
      "459 iter loss. Train : 6.1683497668113025 . Test : 6.184571482958972\n",
      "460 iter loss. Train : 6.1683497668112475 . Test : 6.184571482958972\n",
      "461 iter loss. Train : 6.168349766811238 . Test : 6.184571482958972\n",
      "462 iter loss. Train : 6.168349766811264 . Test : 6.184571482958972\n",
      "463 iter loss. Train : 6.168349766811243 . Test : 6.184571482958972\n",
      "464 iter loss. Train : 6.1683497668112555 . Test : 6.184571482958972\n",
      "465 iter loss. Train : 6.168349766811224 . Test : 6.184571482958972\n",
      "466 iter loss. Train : 6.168349766811253 . Test : 6.184571482958972\n",
      "467 iter loss. Train : 6.168349766811265 . Test : 6.184571482958972\n",
      "468 iter loss. Train : 6.168349766811242 . Test : 6.184571482958972\n",
      "469 iter loss. Train : 6.168349766811275 . Test : 6.184571482958972\n",
      "470 iter loss. Train : 6.16834976681128 . Test : 6.184571482958972\n",
      "471 iter loss. Train : 6.168349766811263 . Test : 6.184571482958972\n",
      "472 iter loss. Train : 6.16834976681126 . Test : 6.184571482958972\n",
      "473 iter loss. Train : 6.168349766811278 . Test : 6.184571482958972\n",
      "474 iter loss. Train : 6.1683497668112395 . Test : 6.184571482958972\n",
      "475 iter loss. Train : 6.16834976681125 . Test : 6.184571482958972\n",
      "476 iter loss. Train : 6.168349766811284 . Test : 6.184571482958972\n",
      "477 iter loss. Train : 6.168349766811214 . Test : 6.184571482958972\n",
      "478 iter loss. Train : 6.168349766811258 . Test : 6.184571482958972\n",
      "479 iter loss. Train : 6.168349766811258 . Test : 6.184571482958972\n",
      "480 iter loss. Train : 6.1683497668112794 . Test : 6.184571482958972\n",
      "481 iter loss. Train : 6.168349766811283 . Test : 6.184571482958972\n",
      "482 iter loss. Train : 6.168349766811279 . Test : 6.184571482958972\n",
      "483 iter loss. Train : 6.1683497668112715 . Test : 6.184571482958972\n",
      "484 iter loss. Train : 6.1683497668112635 . Test : 6.184571482958972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485 iter loss. Train : 6.168349766811308 . Test : 6.184571482958972\n",
      "486 iter loss. Train : 6.1683497668112794 . Test : 6.184571482958972\n",
      "487 iter loss. Train : 6.168349766811232 . Test : 6.184571482958972\n",
      "488 iter loss. Train : 6.1683497668112395 . Test : 6.184571482958972\n",
      "489 iter loss. Train : 6.168349766811261 . Test : 6.184571482958972\n",
      "490 iter loss. Train : 6.168349766811237 . Test : 6.184571482958972\n",
      "491 iter loss. Train : 6.168349766811269 . Test : 6.184571482958972\n",
      "492 iter loss. Train : 6.168349766811268 . Test : 6.184571482958972\n",
      "493 iter loss. Train : 6.168349766811271 . Test : 6.184571482958972\n",
      "494 iter loss. Train : 6.168349766811265 . Test : 6.184571482958972\n",
      "495 iter loss. Train : 6.168349766811249 . Test : 6.184571482958972\n",
      "496 iter loss. Train : 6.168349766811222 . Test : 6.184571482958972\n",
      "497 iter loss. Train : 6.168349766811242 . Test : 6.184571482958972\n",
      "498 iter loss. Train : 6.168349766811235 . Test : 6.184571482958972\n",
      "499 iter loss. Train : 6.168349766811232 . Test : 6.184571482958972\n"
     ]
    }
   ],
   "source": [
    "# Здесь процедура обучения\n",
    "net = MnistNet(0)\n",
    "loss = CrossEntropy()\n",
    "lr = 0.001\n",
    "L_train = []\n",
    "L_test = []\n",
    "for iter in range(500):\n",
    "    L_acc = 0.\n",
    "    sh = list(range(X_train.shape[0])) # больше рандома богу рандома\n",
    "    np.random.shuffle(sh)\n",
    "    for i in range(X_train.shape[0]):\n",
    "        x = X_train[sh[i]]\n",
    "        y = Y_train[sh[i]]\n",
    "        y_h = net.forward(x)\n",
    "        L = loss.forward(y, y_h) #+ net.get_reg_loss()\n",
    "        L_acc += L \n",
    "        dz = loss.backward(1, lr)\n",
    "        dz = net.backward(dz, lr)\n",
    "    L_acc /= Y_train.shape[0]\n",
    "    L_train.append(L_acc)\n",
    "    L_e_acc = 0.\n",
    "    for i in range(X_test.shape[0]):\n",
    "        x = X_test[i]\n",
    "        y = Y_test[i]\n",
    "        y_h = net.forward(x)\n",
    "        L = loss.forward(y, y_h) #+ net.get_reg_loss()\n",
    "        L_e_acc += L\n",
    "    L_e_acc /= Y_test.shape[0]\n",
    "    L_test.append(L_e_acc)\n",
    "    print(\"{} iter loss. Train : {} . Test : {}\".format(iter, L_acc, L_e_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6b7c91ec18>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Здесь необходимо отрисовать графики CrossEntropyLoss для обучающей и валидационной выборок\n",
    "plt.plot(L_train, label=\"train\")\n",
    "plt.plot(L_test, label=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем сабмишшен и заливаем его на kaggle\n",
    "make_submission(X_test_norm, net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
