{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea05145c0d544468b8fda0ca5e27cdb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\titer.\n",
      "Loss:\ttrain:\t1.3878729864562005\ttest:\t0.835197105093811.\n",
      "Accuracy:\ttrain:\t0.7398349206349206\ttest:\t0.7373714285714286.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "\n",
    "# Parameters\n",
    "epochs = 3\n",
    "dense_rlambda = 1e-4\n",
    "dense_lr = 1e-4\n",
    "dropout_p = 0.2\n",
    "norm_epsilon = 1e-4\n",
    "training_epsilon = 1e-3\n",
    "net_list = [784, 50, 50, 10]\n",
    "drop = True\n",
    "last_relu = True\n",
    "br = False\n",
    "\n",
    "# Classes\n",
    "\n",
    "# One-hot encoder for MNIST\n",
    "class OneHotEncoder:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.transform_mapping = np.zeros((10,10))\n",
    "        for i in range(self.transform_mapping.shape[0]):\n",
    "            self.transform_mapping[i][i] = 1.0\n",
    "    \n",
    "    def transform(self, y):\n",
    "        return self.transform_mapping[int(y)]\n",
    "\n",
    "\n",
    "# Fully connected layer\n",
    "class Dense:\n",
    "    \n",
    "    def __init__(self, in_size, out_size):\n",
    "        self.W = np.random.normal(scale=1, size=(out_size, in_size)) * np.sqrt(2 / in_size)\n",
    "        self.b = np.zeros(out_size)\n",
    "        self.rlambda = dense_rlambda\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Remember x for a backward pass\n",
    "        self.x = x\n",
    "        return np.dot(self.W, x) + self.b\n",
    "    \n",
    "    def get_reg_loss(self):\n",
    "        return 0.5 * self.rlambda * (np.linalg.norm(self.W, ord='fro') ** 2)\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        # Calculate gradients by parameters (remember them for debugging)\n",
    "        self.dW = np.outer(dz, self.x)\n",
    "        self.db = dz\n",
    "        # Calculate the input derivative\n",
    "        self.dx = np.matmul(dz, self.W) \n",
    "        # Calculate the gradients from the regularizer\n",
    "        if(self.rlambda != 0):\n",
    "            self.dW += self.rlambda * self.W\n",
    "        # Update weights\n",
    "        self.W = self.W - dense_lr * self.dW\n",
    "        self.b = self.b - dense_lr * self.db\n",
    "        # Return dx to continue the algorithm\n",
    "        return self.dx\n",
    "    \n",
    "\n",
    "# Rectified linear unit\n",
    "class ReLU:\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Remember x for a backward pass\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        dz[self.x < 0] = 0\n",
    "        return dz\n",
    "    \n",
    "    \n",
    "# Normalized exponential function\n",
    "class Softmax:\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        exps = np.exp(x)\n",
    "        return exps / np.sum(exps)\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        sm = self.forward(self.x)\n",
    "        self.lp = (np.eye(sm.shape[0], sm.shape[0]) - sm).T\n",
    "        self.lp2 = sm * self.lp\n",
    "        return np.dot(dz, self.lp2)\n",
    "    \n",
    "    \n",
    "# Measures the performance of a classification model\n",
    "class CrossEntropy:\n",
    "    \n",
    "    def forward(self, y_true, y_hat):\n",
    "        self.y_true = y_true\n",
    "        self.y_hat = y_hat\n",
    "        return -1. * np.sum(y_true * np.log(y_hat))\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        return -1. * dz * self.y_true / self.y_hat\n",
    "    \n",
    "\n",
    "# Regularization technique for reducing overfitting\n",
    "class Dropout:\n",
    "    def __init__(self):\n",
    "        self.p = dropout_p\n",
    "        self.train = True\n",
    "    \n",
    "    def set_train(self, train = True):\n",
    "        self.train = train\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.train:\n",
    "            self.mask = np.ones(*x.shape)\n",
    "            return x\n",
    "        self.mask = (np.random.rand(*x.shape) > self.p) / (1.0 - self.p)\n",
    "        return x * self.mask\n",
    "        \n",
    "    def backward(self, dz):\n",
    "        return dz * self.mask\n",
    "    \n",
    "    \n",
    "# The definition of the network\n",
    "class MnistNet:\n",
    "    \n",
    "    def __init__(self):\n",
    "        for i in range(len(net_list) - 1):\n",
    "            exec(\"self.d{0} = Dense({1}, {2})\".format(i, net_list[i], net_list[i + 1]))\n",
    "            if last_relu and i == len(net_list) - 2:\n",
    "                exec(\"self.m{0} = ReLU()\".format(i))\n",
    "            if i < len(net_list) - 2:\n",
    "                exec(\"self.m{0} = ReLU()\".format(i))\n",
    "                if drop: \n",
    "                    exec(\"self.do%s = Dropout()\" % i)\n",
    "        self.s = Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        net = self.d0.forward(x)\n",
    "        net = self.m0.forward(net)\n",
    "        if drop:\n",
    "            net = self.do0.forward(net)\n",
    "        for i in range(len(net_list) - 2):\n",
    "            net = eval(\"self.d{0}.forward(net)\".format(i + 1))\n",
    "            if last_relu and i == len(net_list) - 3:\n",
    "                net = eval(\"self.m{0}.forward(net)\".format(i + 1))\n",
    "            if i < len(net_list) - 3:\n",
    "                net = eval(\"self.m{0}.forward(net)\".format(i + 1))\n",
    "                if drop: \n",
    "                    net = eval(\"self.do{0}.forward(net)\".format(i + 1))\n",
    "        net = self.s.forward(net)\n",
    "        return net\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        dz = self.s.backward(dz)\n",
    "        for i in range(len(net_list) - 1):\n",
    "            if drop and i > 0:\n",
    "                dz = eval(\"self.do{0}.backward(dz)\".format(len(net_list) - (i + 2)))\n",
    "            if i > 0:\n",
    "                dz = eval(\"self.m{0}.backward(dz)\".format(len(net_list) - (i + 2)))\n",
    "            if last_relu and i == 0:\n",
    "                dz = eval(\"self.m{0}.backward(dz)\".format(len(net_list) - (i + 2)))\n",
    "            dz = eval(\"self.d{0}.backward(dz)\".format(len(net_list) - (i + 2)))\n",
    "        return dz\n",
    "    \n",
    "    \n",
    "# Functions\n",
    "\n",
    "def data_normalization(data):\n",
    "    data = data.astype('float')\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    return (data - mean) / (std + norm_epsilon)\n",
    "\n",
    "# Y_test in the non-one-hot format\n",
    "def compute_acc(X_test, Y_test, net):\n",
    "    acc = 0.0\n",
    "    for i in range(X_test.shape[0]):\n",
    "        y_h = net.forward(X_test[i])\n",
    "        y = np.argmax(y_h)\n",
    "        if(y == Y_test[i]):\n",
    "            acc += 1.0\n",
    "    return acc / Y_test.shape[0]\n",
    "\n",
    "def make_submission(X_test, net, fname=\"submission.csv\"):\n",
    "    with open(fname,'w') as fout:\n",
    "        fout.write('Id,Category')\n",
    "        for i in range(X_test.shape[0]):\n",
    "            y_h = net.forward(X_test[i])\n",
    "            y = np.argmax(y_h)\n",
    "            fout.write(\"\\n{},{}\".format(i, int(y)))\n",
    "            \n",
    "\n",
    "# Main code block\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "with open('data_train.pickle','rb') as fin:\n",
    "    train_data = pickle.load(fin)\n",
    "with open('data_test_no_labels.pickle','rb') as fin:\n",
    "    test_data = pickle.load(fin)\n",
    "    \n",
    "X_train = train_data['data']\n",
    "Y_train = train_data['target']\n",
    "Y_train_oh = np.array(list(map(lambda x : encoder.transform(x), Y_train)))\n",
    "X_test_no_lab = test_data['data']\n",
    "\n",
    "# Normalization of the input data\n",
    "X_train = data_normalization(X_train)\n",
    "\n",
    "# Splitting data on the training and the test samples\n",
    "X_train, X_test, Y_train, Y_test, Y_train_noh, Y_test_noh = train_test_split(X_train, Y_train_oh, Y_train, random_state=10)\n",
    "\n",
    "# Initialization of objects\n",
    "net = MnistNet()\n",
    "loss = CrossEntropy()\n",
    "L_train, L_test, A_train, A_test = [], [], [], []\n",
    "\n",
    "# Training\n",
    "for iter in tqdm.tqdm_notebook(range(epochs)):\n",
    "    L_acc = 0.\n",
    "    sh = list(range(X_train.shape[0]))\n",
    "    np.random.shuffle(sh)\n",
    "    for i in range(X_train.shape[0]):\n",
    "        x = X_train[sh[i]]\n",
    "        y = Y_train[sh[i]]\n",
    "        y_h = net.forward(x)\n",
    "        L = loss.forward(y, y_h)\n",
    "        L_acc += L \n",
    "        dz = loss.backward(1.)\n",
    "        dz = net.backward(dz)\n",
    "    L_acc /= Y_train.shape[0]\n",
    "    L_train.append(L_acc)\n",
    "    L_e_acc = 0.\n",
    "    for i in range(X_test.shape[0]):\n",
    "        x = X_test[i]\n",
    "        y = Y_test[i]\n",
    "        y_h = net.forward(x)\n",
    "        L = loss.forward(y, y_h)\n",
    "        L_e_acc += L\n",
    "    L_e_acc /= Y_test.shape[0]\n",
    "    L_test.append(L_e_acc)\n",
    "    A_acc = compute_acc(X_train, Y_train_noh, net)\n",
    "    A_e_acc = compute_acc(X_test, Y_test_noh, net)\n",
    "    A_test.append(A_e_acc)\n",
    "    A_train.append(A_acc)\n",
    "    print(\"{}\\titer.\\nLoss:\\ttrain:\\t{}\\ttest:\\t{}.\\nAccuracy:\\ttrain:\\t{}\\ttest:\\t{}.\".format(iter, L_acc, L_e_acc, A_acc, A_e_acc))\n",
    "\n",
    "# Plotting cross-entropy loss and accuracy \n",
    "figure(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(L_train, label=\"train\")\n",
    "plt.plot(L_test, label=\"test\")\n",
    "plt.xlabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(A_train, label='train')\n",
    "plt.plot(A_test, label='test')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17500, 784)\n"
     ]
    }
   ],
   "source": [
    "# формируем сабмишшен и заливаем его на kaggle\n",
    "\n",
    "X_test_norm = data_normalization(X_test_no_lab)\n",
    "print(X_test_no_lab.shape)\n",
    "make_submission(X_test_norm, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
