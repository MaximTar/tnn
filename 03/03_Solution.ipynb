{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "from matplotlib.pyplot import figure\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of own convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_conv2d(x, kernel, stride=(1,1), padding=(0,0), bias=None):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    :param  x: input tensor 4d, type tensor.FloatTensor, BxCxHxW,\n",
    "    :param  kernel: input kernel tensor 3d, type tensor.FloatTensor, CxHxW,\n",
    "    :param  stride: tuple - stride parameters, set in HxW format,\n",
    "    :param  padding:  tuple - padding parameters, set in HxW format,\n",
    "    :param  bias : the input tensor bias is added to the output tensor,\n",
    "    \n",
    "    where:\n",
    "    B is the batch or the number of input,\n",
    "    C is the number of image channels,\n",
    "    H is the height of image,\n",
    "    W is is the width of the image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # getting input parameters\n",
    "    in_s = x.size()\n",
    "    if x.dim() == 3:\n",
    "        b_in, c_in, h_in, w_in = 1, in_s[0], in_s[1], in_s[2]\n",
    "    elif x.dim() == 4:\n",
    "        b_in, c_in, h_in, w_in = in_s[0], in_s[1], in_s[2], in_s[3]\n",
    "    else:\n",
    "        raise Exception(\"ERROR: Wrong input tensor. Input tensor must be 3d or 4d.\")\n",
    "        \n",
    "    k_s = kernel.size()\n",
    "    k_c, k_h, k_w = k_s[0], k_s[1], k_s[2]\n",
    "    \n",
    "    # output size calculation\n",
    "    h_out = (h_in + 2 * padding[0] - k_h) / stride[0] + 1\n",
    "    w_out = (w_in + 2 * padding[1] - k_w) / stride[1] + 1\n",
    "    \n",
    "    if not h_out.is_integer() or not w_out.is_integer():\n",
    "        print(\"Wrong output dimension. H_out = {}, W_out = {}.\".format(h_out, w_out))\n",
    "        print(\"Some input units will be left out.\")\n",
    "        h_out = (h_in + 2 * padding[0] - k_h) // stride[0] + 1\n",
    "        w_out = (w_in + 2 * padding[1] - k_w) // stride[1] + 1\n",
    "    h_out = int(h_out)\n",
    "    w_out = int(w_out)\n",
    "    print(\"Output size: H_out = {}, W_out = {}\".format(h_out, w_out))\n",
    "        \n",
    "    # adding padding\n",
    "    x = torch.from_numpy(np.pad(x.numpy(),\n",
    "                                ((0, 0), (padding[0],padding[0]), (padding[1],padding[1])),\n",
    "                                mode='constant'))\n",
    "    \n",
    "    # output tensor\n",
    "    x_out = torch.zeros(h_out, w_out)\n",
    "    \n",
    "    # TODO: SMTH WRONG WITH STRIDE. DON'T KNOW...\n",
    "    # iterating over input tensor\n",
    "    #for b in range(b_in):\n",
    "    for c in range(c_in):\n",
    "        #for h in range(0, h_in + 2 * padding[0] - k_h + 1, stride[0]):\n",
    "        for h in range(h_out):\n",
    "            #for w in range(0, w_in + 2 * padding[1] - k_w + 1, stride[1]):\n",
    "            for w in range(w_out):\n",
    "                res = 0\n",
    "                for kc in range(k_c):\n",
    "                    for kh in range(k_h):\n",
    "                        for kw in range(k_w):\n",
    "                            #res += x[c][h + kh][w + kw] * kernel[kc][kh][kw]\n",
    "                            px += x[c][h * stride[0] + kh][w * stride[1] + kw] * kernel[kc][kh][kw]\n",
    "                #x_out[int(h/stride[0])][int(w/stride[1])] += (res / k_c)\n",
    "                x_out[h][w] += (px / k_c)\n",
    "                            \n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, affine=True, beta=0.9, epsilon=1e-8):\n",
    "        \"\"\"       \n",
    "         BatchNorm Initialization\n",
    "         If the affine flag is set, then gamma and b matrices must be initialized\n",
    "         to implement affine transformations in the process of training and testing.\n",
    "         The parameters to be learned are set as a tensor of the corresponding dimension and save their\n",
    "         self.weight\n",
    "         self.bias\n",
    "         \n",
    "        : param in_channels: the number of input channels of the previous layer\n",
    "        : param affine: whether to make affine transformation in the learning process\n",
    "        : param beta: smoothing parameter (momentum)\n",
    "        : param epsilon: parameter excluding division by zero\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_c = in_channels\n",
    "        self.momentum = beta\n",
    "        self.eps = epsilon\n",
    "        \n",
    "        if affine:\n",
    "            self.gamma = torch.Tensor(1, self.in_c, 1, 1).uniform_()\n",
    "            self.b = torch.Tensor(1, self.in_c, 1, 1).fill_(0)\n",
    "        else:\n",
    "            self.gamma = torch.Tensor(1, self.in_c, 1, 1).fill_(1)\n",
    "            self.b = torch.Tensor(1, self.in_c, 1, 1).fill_(0)\n",
    "        \n",
    "        self.runing_mean = torch.Tensor(self.in_c).fill_(0)\n",
    "        self.runing_var = torch.Tensor(self.in_c).fill_(1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Считаем параметры нормализации в режиме обучения, и нормализуем x в обоих режимах\n",
    "        используем для расчета параметры gamma и b КАК обучаемые, т.е. учитываем, \n",
    "        что эти параметры должны быть обучены в процессе тренировки. \n",
    "        We consider the normalization parameters in the learning mode, and normalize x in both modes,\n",
    "        use the gamma and b parameters for the calculation as the trainees,\n",
    "        i.e. we take into account that these parameters must be trained in the process of training.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.training:\n",
    "            mean = x.contiguous().view(x.size(1), -1).mean(-1)\n",
    "            var = x.contiguous().view(x.size(1), -1).var(-1)\n",
    "\n",
    "            self.runing_mean = self.momentum * self.runing_mean + (1.0 - self.momentum) * mean\n",
    "            self.runing_var = self.momentum * self.runing_var + (1.0 - self.momentum) * var\n",
    "        else :\n",
    "            mean = self.runing_mean\n",
    "            var = self.runing_var\n",
    "        \n",
    "        t_mean = torch.t(mean.unsqueeze(0)).unsqueeze(0).unsqueeze(-1).expand_as(x)\n",
    "        t_var = torch.t(var.unsqueeze(0)).unsqueeze(0).unsqueeze(-1).expand_as(x)\n",
    "        \n",
    "        x_hat = self.gamma * (x - t_mean) / np.sqrt(t_var + self.eps) + self.b\n",
    "            \n",
    "        return x_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.5417)\n",
      "tensor([[[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.],\n",
      "         [13., 14., 15., 16.]],\n",
      "\n",
      "        [[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.],\n",
      "         [13., 14., 15., 16.]],\n",
      "\n",
      "        [[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.],\n",
      "         [13., 14., 15., 18.]]])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 3, 4, 4])\n",
      "tensor([[[[8.5000, 8.5000, 8.5000, 8.5000],\n",
      "          [8.5000, 8.5000, 8.5000, 8.5000],\n",
      "          [8.5000, 8.5000, 8.5000, 8.5000],\n",
      "          [8.5000, 8.5000, 8.5000, 8.5000]],\n",
      "\n",
      "         [[8.5000, 8.5000, 8.5000, 8.5000],\n",
      "          [8.5000, 8.5000, 8.5000, 8.5000],\n",
      "          [8.5000, 8.5000, 8.5000, 8.5000],\n",
      "          [8.5000, 8.5000, 8.5000, 8.5000]],\n",
      "\n",
      "         [[8.6250, 8.6250, 8.6250, 8.6250],\n",
      "          [8.6250, 8.6250, 8.6250, 8.6250],\n",
      "          [8.6250, 8.6250, 8.6250, 8.6250],\n",
      "          [8.6250, 8.6250, 8.6250, 8.6250]]]])\n",
      "END\n",
      "tensor([[[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.],\n",
      "         [13., 14., 15., 16.]],\n",
      "\n",
      "        [[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.],\n",
      "         [13., 14., 15., 16.]],\n",
      "\n",
      "        [[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.],\n",
      "         [13., 14., 15., 18.]]])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "Wrong output dimension. H_out = 3.5, W_out = 3.5.\n",
      "Some input units will be left out.\n",
      "New output sizes: H_out = 3, W_out = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  3.,  18.,  21.],\n",
       "        [ 45., 162., 135.],\n",
       "        [ 66., 216., 164.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([[\n",
    "    [1,2,3,4],\n",
    "    [5,6,7,8],\n",
    "    [9,10,11,12],\n",
    "    [13,14,15,16]\n",
    "],[\n",
    "    [1,2,3,4],\n",
    "    [5,6,7,8],\n",
    "    [9,10,11,12],\n",
    "    [13,14,15,16]\n",
    "],[\n",
    "    [1,2,3,4],\n",
    "    [5,6,7,8],\n",
    "    [9,10,11,12],\n",
    "    [13,14,15,18]\n",
    "]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(torch.mean(t))\n",
    "print(t)\n",
    "m = t.contiguous().view(t.size(0), -1).mean(-1)\n",
    "print(m.unsqueeze(0).size())\n",
    "m = torch.t(m.unsqueeze(0)).unsqueeze(0).unsqueeze(-1).expand_as(t.unsqueeze(0))\n",
    "print(m.size())\n",
    "print(m)\n",
    "#m = m.unsqueeze(0)\n",
    "#print(m.size())\n",
    "#print(m)\n",
    "#m = m.unsqueeze(-1)\n",
    "#print(m.size())\n",
    "#print(m)\n",
    "#print(m.expand_as(t.unsqueeze(0)))\n",
    "#print(t.view(t.size(0), -1))\n",
    "#print(t.squeeze(2))\n",
    "#print(torch.mean(t.squeeze()))\n",
    "#print(torch.mean(t[2]))\n",
    "print(\"END\")\n",
    "\n",
    "filter_kernel = torch.FloatTensor(3,4,4).fill_(0)\n",
    "filter_kernel[1,:,:] = 1\n",
    "\n",
    "print(t)\n",
    "#print(t.size())\n",
    "#print(t.numel())\n",
    "\n",
    "filter_kernel = torch.FloatTensor(1,3,3).fill_(1)\n",
    "#filter_kernel[:,1,1] = 8\n",
    "\n",
    "print(filter_kernel)\n",
    "\n",
    "my_conv2d(t, filter_kernel, stride=(2,2), padding=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
