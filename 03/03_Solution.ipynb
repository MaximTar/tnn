{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "from matplotlib.pyplot import figure\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of own convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_conv2d(x, kernel, stride=(1,1), padding=(0,0), bias=None):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    :param  x: input tensor 4d, type tensor.FloatTensor, BxCxHxW,\n",
    "    :param  kernel: input kernel tensor 3d, type tensor.FloatTensor, CxHxW,\n",
    "    :param  stride: tuple - stride parameters, set in HxW format,\n",
    "    :param  padding:  tuple - padding parameters, set in HxW format,\n",
    "    :param  bias : the input tensor bias is added to the output tensor,\n",
    "    \n",
    "    where:\n",
    "    B is the batch or the number of input,\n",
    "    C is the number of image channels,\n",
    "    H is the height of image,\n",
    "    W is is the width of the image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # getting input parameters\n",
    "    in_s = x.size()\n",
    "    if x.dim() == 3:\n",
    "        b_in, c_in, h_in, w_in = 1, in_s[0], in_s[1], in_s[2]\n",
    "    elif x.dim() == 4:\n",
    "        b_in, c_in, h_in, w_in = in_s[0], in_s[1], in_s[2], in_s[3]\n",
    "    else:\n",
    "        raise Exception(\"ERROR: Wrong input tensor. Input tensor must be 3d or 4d.\")\n",
    "        \n",
    "    k_s = kernel.size()\n",
    "    k_c, k_h, k_w = k_s[0], k_s[1], k_s[2]\n",
    "    \n",
    "    # output size calculation\n",
    "    h_out = (h_in + 2 * padding[0] - k_h) / stride[0] + 1\n",
    "    w_out = (w_in + 2 * padding[1] - k_w) / stride[1] + 1\n",
    "    \n",
    "    if not h_out.is_integer() or not w_out.is_integer():\n",
    "        print(\"Wrong output dimension. H_out = {}, W_out = {}.\".format(h_out, w_out))\n",
    "        print(\"Some input units will be left out.\")\n",
    "        h_out = (h_in + 2 * padding[0] - k_h) // stride[0] + 1\n",
    "        w_out = (w_in + 2 * padding[1] - k_w) // stride[1] + 1\n",
    "    h_out = int(h_out)\n",
    "    w_out = int(w_out)\n",
    "    print(\"Output size: H_out = {}, W_out = {}\".format(h_out, w_out))\n",
    "        \n",
    "    # adding padding\n",
    "    x = torch.from_numpy(np.pad(x.numpy(),\n",
    "                                ((0, 0), (padding[0],padding[0]), (padding[1],padding[1])),\n",
    "                                mode='constant'))\n",
    "    \n",
    "    # output tensor\n",
    "    x_out = torch.zeros(h_out, w_out)\n",
    "    \n",
    "    # TODO: SMTH WRONG WITH STRIDE. DON'T KNOW...\n",
    "    # iterating over input tensor\n",
    "    #for b in range(b_in):\n",
    "    for c in range(c_in):\n",
    "        #for h in range(0, h_in + 2 * padding[0] - k_h + 1, stride[0]):\n",
    "        for h in range(h_out):\n",
    "            #for w in range(0, w_in + 2 * padding[1] - k_w + 1, stride[1]):\n",
    "            for w in range(w_out):\n",
    "                res = 0\n",
    "                for kc in range(k_c):\n",
    "                    for kh in range(k_h):\n",
    "                        for kw in range(k_w):\n",
    "                            #res += x[c][h + kh][w + kw] * kernel[kc][kh][kw]\n",
    "                            px += x[c][h * stride[0] + kh][w * stride[1] + kw] * kernel[kc][kh][kw]\n",
    "                #x_out[int(h/stride[0])][int(w/stride[1])] += (res / k_c)\n",
    "                x_out[h][w] += (px / k_c)\n",
    "                            \n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, affine=True, beta=0.9, epsilon=1e-8):\n",
    "        \"\"\"       \n",
    "         BatchNorm Initialization\n",
    "         If the affine flag is set, then gamma and b matrices must be initialized\n",
    "         to implement affine transformations in the process of training and testing.\n",
    "         The parameters to be learned are set as a tensor of the corresponding dimension and save their\n",
    "         self.weight\n",
    "         self.bias\n",
    "         \n",
    "        : param in_channels: the number of input channels of the previous layer\n",
    "        : param affine: whether to make affine transformation in the learning process\n",
    "        : param beta: smoothing parameter (momentum)\n",
    "        : param epsilon: parameter excluding division by zero\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_c = in_channels\n",
    "        self.momentum = beta\n",
    "        self.eps = epsilon\n",
    "        \n",
    "        self.gamma = torch.Tensor(1, self.in_c, 1, 1).fill_(1)\n",
    "        self.b = torch.Tensor(1, self.in_c, 1, 1).fill_(0)\n",
    "        \n",
    "        self.mean = torch.Tensor(1, self.in_c, 1, 1).fill_(0)\n",
    "        self.var = torch.Tensor(1, self.in_c, 1, 1).fill_(1)\n",
    "        self.runing_mean = torch.Tensor(1, self.in_c, 1, 1).fill_(0)\n",
    "        self.runing_var = torch.Tensor(1, self.in_c, 1, 1).fill_(1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Считаем параметры нормализации в режиме обучения, и нормализуем x в обоих режимах\n",
    "        используем для расчета параметры gamma и b КАК обучаемые, т.е. учитываем, \n",
    "        что эти параметры должны быть обучены в процессе тренировки. \n",
    "        \"\"\"\n",
    "        \n",
    "        self.mean = torch.Tensor(1, self.in_c, 1, 1)\n",
    "        self.var = torch.Tensor(1, self.in_c, 1, 1)\n",
    "        for i in range(self.in_c):\n",
    "            self.mean[0][i] = torch.mean(x[0][i])\n",
    "            self.var[0][i] = torch.var(x[0][i])\n",
    "        self.mean.expand_as(x)\n",
    "        self.var.expand_as(x)\n",
    "        \n",
    "        if self.training:\n",
    "            self.runing_mean = self.momentum * self.runing_mean.expand_as(x) + (1 - self.momentum) * self.mean\n",
    "            self.runing_var = self.momentum * self.runing_var.expand_as(x) + (1 - self.momentum) * self.var\n",
    "            x_norm = (x - self.runing_mean) / np.sqrt(self.runing_var + self.eps)\n",
    "        else:\n",
    "            x_norm = (x - self.mean) / np.sqrt(self.var + self.eps)\n",
    "        \n",
    "        x_hat = self.gamma * x_norm + self.b\n",
    "        \n",
    "        return x_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
