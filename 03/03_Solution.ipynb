{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "from matplotlib.pyplot import figure\n",
    "import torch\n",
    "import torchvision as tv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of own convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_conv2d(x,\n",
    "              kernel,\n",
    "              stride=(1,1), padding=(0,0), bias=None):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    :param  x: input tensor 4d, type tensor.FloatTensor, BxCxHxW,\n",
    "    :param  kernel: input kernel tensor 3d, type tensor.FloatTensor, CxHxW,\n",
    "    :param  stride: tuple - stride parameters, set in HxW format,\n",
    "    :param  padding:  tuple - padding parameters, set in HxW format,\n",
    "    :param  bias : the input tensor bias is added to the output tensor,\n",
    "    \n",
    "    where:\n",
    "    B is the batch or the number of input,\n",
    "    C is the number of image channels,\n",
    "    H is the height of image,\n",
    "    W is is the width of the image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # getting input parameters\n",
    "    print(x.size())\n",
    "    in_s = x.size()\n",
    "    if x.dim() == 3:\n",
    "        b_in = 1\n",
    "        c_in = in_s[0]\n",
    "        h_in = in_s[1]\n",
    "        w_in = in_s[2]\n",
    "    elif x.dim() == 4:\n",
    "        b_in = in_s[0]\n",
    "        c_in = in_s[1]\n",
    "        h_in = in_s[2]\n",
    "        w_in = in_s[3]\n",
    "    else:\n",
    "        raise Exception(\"ERROR: Wrong input tensor. Input tensor must be 3d or 4d.\")\n",
    "        \n",
    "    k_s = kernel.size()\n",
    "    k_c = k_s[0]\n",
    "    k_h = k_s[1]\n",
    "    k_w = k_s[2]\n",
    "    \n",
    "    # output size calculation\n",
    "    h_out = (h_in + 2 * padding[0] - k_h) / stride[0] + 1\n",
    "    w_out = (w_in + 2 * padding[1] - k_w) / stride[1] + 1\n",
    "    \n",
    "    if not h_out.is_integer() or not w_out.is_integer():\n",
    "        print(\"Wrong output dimension. H_out = {}, W_out = {}. Some input units will be left out.\".format(h_out, w_out))\n",
    "        h_out = (h_in + 2 * padding[0] - k_h) // stride[0] + 1\n",
    "        w_out = (w_in + 2 * padding[1] - k_w) // stride[1] + 1\n",
    "        print(\"New output sizes: H_out = {}, W_out = {}\".format(h_out, w_out))\n",
    "    #else:\n",
    "    h_out = int(h_out)\n",
    "    w_out = int(w_out)\n",
    "        \n",
    "    # adding padding\n",
    "    x = torch.from_numpy(np.pad(x.numpy(),\n",
    "                                ((0, 0), (padding[0],padding[0]), (padding[1],padding[1])),\n",
    "                                mode='constant'))\n",
    "    \n",
    "    # output tensor\n",
    "    x_out = torch.zeros(h_out, w_out)\n",
    "    print(x_out.size())\n",
    "    \n",
    "    # iterating over input tensor\n",
    "    #for b in range(b_in):\n",
    "    for c in range(c_in):\n",
    "        for h in range(0, h_in + 2 * padding[0] - k_h + 1, stride[0]):\n",
    "            #print(\"h = \", h)\n",
    "            for w in range(0, w_in + 2 * padding[1] - k_w + 1, stride[1]):\n",
    "                res = 0\n",
    "                for kc in range(k_c):\n",
    "                    for kh in range(k_h):\n",
    "                        for kw in range(k_w):\n",
    "                            res += x[c][h + kh][w + kw] * kernel[kc][kh][kw]\n",
    "                x_out[int(h/stride[0])][int(w/stride[1])] += (res / k_c)\n",
    "                            \n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.],\n",
      "         [13., 14., 15., 16.]],\n",
      "\n",
      "        [[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.],\n",
      "         [13., 14., 15., 16.]],\n",
      "\n",
      "        [[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.],\n",
      "         [13., 14., 15., 16.]]])\n",
      "tensor(1.)\n",
      "[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  2.  3.  4.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  5.  6.  7.  8.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  9. 10. 11. 12.  0.  0.  0.]\n",
      "  [ 0.  0.  0. 13. 14. 15. 16.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  2.  3.  4.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  5.  6.  7.  8.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  9. 10. 11. 12.  0.  0.  0.]\n",
      "  [ 0.  0.  0. 13. 14. 15. 16.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  2.  3.  4.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  5.  6.  7.  8.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  9. 10. 11. 12.  0.  0.  0.]\n",
      "  [ 0.  0.  0. 13. 14. 15. 16.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([[\n",
    "    [1,2,3,4],\n",
    "    [5,6,7,8],\n",
    "    [9,10,11,12],\n",
    "    [13,14,15,16]\n",
    "],[\n",
    "    [1,2,3,4],\n",
    "    [5,6,7,8],\n",
    "    [9,10,11,12],\n",
    "    [13,14,15,16]\n",
    "],[\n",
    "    [1,2,3,4],\n",
    "    [5,6,7,8],\n",
    "    [9,10,11,12],\n",
    "    [13,14,15,16]\n",
    "]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "print(t)\n",
    "print(t[0][0][0])\n",
    "#print(t.size())\n",
    "#print(t.numel())\n",
    "\n",
    "padding = (2, 3)\n",
    "#x_padded = np.pad(t[0], ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "t = np.pad(t, ((0, 0), (padding[0],padding[0]), (padding[1],padding[1])), mode='constant')\n",
    "\n",
    "print(t)\n",
    "print(t[0][2][3])\n",
    "\n",
    "#filter_kernel = torch.FloatTensor(3,1,1).fill_(-1)\n",
    "#filter_kernel[:,1,1] = 8\n",
    "\n",
    "#print(filter_kernel)\n",
    "\n",
    "#my_conv2d(t, filter_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
